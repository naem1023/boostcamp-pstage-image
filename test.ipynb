{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "\n",
    "import glob"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# 테스트 데이터셋 폴더 경로를 지정해주세요.\n",
    "test_dir = '/opt/ml/input/data/eval'\n",
    "test_dir_glob = glob.glob(test_dir + '/images/**/*')\n",
    "## png, jpg 파일 동시에 존재\n",
    "train_dir = '/opt/ml/input/data/train'\n",
    "train_dir_glob = glob.glob(train_dir + '/images/**/*')\n",
    "\n",
    "test_csv = '/opt/ml/input/data/train/test.csv'\n",
    "train_csv = '/opt/ml/input/data/train/train.csv'\n",
    "\n",
    "count = 0\n",
    "for files in train_dir_glob:\n",
    "    if count == 10:\n",
    "        break\n",
    "    if files[-3:] == 'png':\n",
    "        print(files)\n",
    "        count += 1"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/opt/ml/input/data/train/images/006224_male_Asian_20/mask5.png\n",
      "/opt/ml/input/data/train/images/006224_male_Asian_20/mask2.png\n",
      "/opt/ml/input/data/train/images/006224_male_Asian_20/mask1.png\n",
      "/opt/ml/input/data/train/images/006224_male_Asian_20/mask4.png\n",
      "/opt/ml/input/data/train/images/006224_male_Asian_20/mask3.png\n",
      "/opt/ml/input/data/train/images/006224_male_Asian_20/normal.png\n",
      "/opt/ml/input/data/train/images/006224_male_Asian_20/incorrect_mask.png\n",
      "/opt/ml/input/data/train/images/006163_female_Asian_18/mask5.png\n",
      "/opt/ml/input/data/train/images/006163_female_Asian_18/mask2.png\n",
      "/opt/ml/input/data/train/images/006163_female_Asian_18/mask1.png\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "train_pd = pd.read_csv(train_csv)\n",
    "test_pd = pd.read_csv(test_csv)\n",
    "print(train_pd)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "          id  gender   race  age                    path\n",
      "0     000001  female  Asian   45  000001_female_Asian_45\n",
      "1     000002  female  Asian   52  000002_female_Asian_52\n",
      "2     000004    male  Asian   54    000004_male_Asian_54\n",
      "3     000005  female  Asian   58  000005_female_Asian_58\n",
      "4     000006  female  Asian   59  000006_female_Asian_59\n",
      "...      ...     ...    ...  ...                     ...\n",
      "2695  006954    male  Asian   19    006954_male_Asian_19\n",
      "2696  006955    male  Asian   19    006955_male_Asian_19\n",
      "2697  006956    male  Asian   19    006956_male_Asian_19\n",
      "2698  006957    male  Asian   20    006957_male_Asian_20\n",
      "2699  006959    male  Asian   19    006959_male_Asian_19\n",
      "\n",
      "[2700 rows x 5 columns]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "mask\n",
    "    wear: 0\n",
    "    incorrect: 1\n",
    "    not wear: 2\n",
    "gender\n",
    "    male: 0\n",
    "    female: 1\n",
    "age\n",
    "    <30: 0\n",
    "    >=30 and <60: 1\n",
    "    >=60: 2\n",
    "\"\"\"\n",
    "\n",
    "def get_label(mask, gender, age):\n",
    "    if mask \n",
    "train_dir_glob"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class MaskDataset(Dataset):\n",
    "    def __init__(self, imgaes, train=True):\n",
    "        self.images = images\n",
    "        self.features = list(set(self.data.columns) - set(drop_features) - set('Survived'))\n",
    "        self.classes = ['Survived']\n",
    "        self.X = self.data.loc[:, self.features].values\n",
    "        self.y = self.data.loc[:, self.classes].values\n",
    "        ################################################################################\n",
    "\n",
    "    def __len__(self):\n",
    "        len_dataset=None\n",
    "        ######################################TODO######################################\n",
    "        len_dataset = self.X.shape[0]\n",
    "        ################################################################################\n",
    "        return len_dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X, y = None, None\n",
    "        # print(self.X)\n",
    "        ######################################TODO######################################\n",
    "        X = self.X[idx]\n",
    "        y = self.y[idx]\n",
    "        ################################################################################\n",
    "        return torch.tensor(X), torch.tensor(y)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}