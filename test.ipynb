{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def create_datasets(batch_size):\n",
    "    # trainning set 중 validation 데이터로 사용할 비율\n",
    "    valid_size = 0.2\n",
    "\n",
    "    # torch.FloatTensor로 변환\n",
    "    transform = transforms.ToTensor()\n",
    "\n",
    "    # choose the training and test datasets\n",
    "    train_data = datasets.MNIST(root = 'data', train = True,\n",
    "                                download = True, transform = transform)\n",
    "    test_data = datasets.MNIST(root = 'data', train = False,\n",
    "                                download = True, transform = transform)\n",
    "\n",
    "    # validation으로 사용할 trainning indices를 얻는다.\n",
    "    num_train = len(train_data)\n",
    "    indices = list(range(num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "    # trainning, validation batch를 얻기 위한 sampler정의\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    # load training data in batches\n",
    "    train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                               batch_size=batch_size,\n",
    "                                               sampler=train_sampler,\n",
    "                                               num_workers=0)\n",
    "\n",
    "    # load validation data in batches\n",
    "    valid_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                               batch_size=batch_size,\n",
    "                                               sampler=valid_sampler,\n",
    "                                               num_workers=0)\n",
    "\n",
    "    # load test data in batches\n",
    "    test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                              batch_size=batch_size,\n",
    "                                              num_workers=0)\n",
    "\n",
    "    return train_loader, test_loader, valid_loader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128,10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # image input을 펼쳐준다.\n",
    "        x = x.view(-1, 28*28)\n",
    "\n",
    "        # 은닉층을 추가하고 활성화 함수로 relu 사용\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # 은닉층을 추가하고 활성화 함수로 relu 사용\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # 출력층 추가\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# initialize the NN / 모델 확인\n",
    "model = Net()\n",
    "print(model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# loss function의 정의(CrossEntropyLoss)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer 정의(Adam)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"주어진 patience 이후로 validation loss가 개선되지 않으면 학습을 조기 중지\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): validation loss가 개선된 후 기다리는 기간\n",
    "                            Default: 7\n",
    "            verbose (bool): True일 경우 각 validation loss의 개선 사항 메세지 출력\n",
    "                            Default: False\n",
    "            delta (float): 개선되었다고 인정되는 monitered quantity의 최소 변화\n",
    "                            Default: 0\n",
    "            path (str): checkpoint저장 경로\n",
    "                            Default: 'checkpoint.pt'\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''validation loss가 감소하면 모델을 저장한다.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def train_model(model, batch_size, patience, n_epochs):\n",
    "\n",
    "    # 모델이 학습되는 동안 trainning loss를 track\n",
    "    train_losses = []\n",
    "    # 모델이 학습되는 동안 validation loss를 track\n",
    "    valid_losses = []\n",
    "    # epoch당 average training loss를 track\n",
    "    avg_train_losses = []\n",
    "    # epoch당 average validation loss를 track\n",
    "    avg_valid_losses = []\n",
    "\n",
    "    # early_stopping object의 초기화\n",
    "    early_stopping = EarlyStopping(patience = patience, verbose = True)\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train() # prep model for training\n",
    "        for batch, (data, target) in enumerate(train_loader, 1):\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: 입력된 값을 모델로 전달하여 예측 출력 계산\n",
    "            output = model(data)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: 모델의 파라미터와 관련된 loss의 그래디언트 계산\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # record training loss\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "\n",
    "        ######################\n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval() # prep model for evaluation\n",
    "        for data , target in valid_loader :\n",
    "            # forward pass: 입력된 값을 모델로 전달하여 예측 출력 계산\n",
    "            output = model(data)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            # record validation loss\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "        # print 학습/검증 statistics\n",
    "        # epoch당 평균 loss 계산\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "\n",
    "        epoch_len = len(str(n_epochs))\n",
    "\n",
    "\n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f}')\n",
    "\n",
    "        print(print_msg)\n",
    "\n",
    "        # clear lists to track next epoch\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "\n",
    "        # early_stopping는 validation loss가 감소하였는지 확인이 필요하며,\n",
    "        # 만약 감소하였을경우 현제 모델을 checkpoint로 만든다.\n",
    "        early_stopping(valid_loss, model)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "   # best model이 저장되어있는 last checkpoint를 로드한다.\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "\n",
    "    return  model, avg_train_losses, avg_valid_losses"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "31b2b117d925460da48098fd0fb28d20"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8563da13ea254239b4ff99b58a073030"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9c6fe56484824eb0bcacc0a15c6b1385"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "323a12cdb97d4c219bf73eefc2db2bb3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\envs\\torch\\lib\\site-packages\\torchvision\\datasets\\mnist.py:335: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed).view(length, num_rows, num_cols)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "[  1/100] train_loss: 0.82333 valid_loss: 0.29947\n",
      "Validation loss decreased (inf --> 0.299474).  Saving model ...\n",
      "[  2/100] train_loss: 0.36800 valid_loss: 0.21973\n",
      "Validation loss decreased (0.299474 --> 0.219734).  Saving model ...\n",
      "[  3/100] train_loss: 0.29981 valid_loss: 0.18481\n",
      "Validation loss decreased (0.219734 --> 0.184809).  Saving model ...\n",
      "[  4/100] train_loss: 0.25914 valid_loss: 0.16143\n",
      "Validation loss decreased (0.184809 --> 0.161426).  Saving model ...\n",
      "[  5/100] train_loss: 0.23563 valid_loss: 0.14466\n",
      "Validation loss decreased (0.161426 --> 0.144659).  Saving model ...\n",
      "[  6/100] train_loss: 0.21477 valid_loss: 0.13740\n",
      "Validation loss decreased (0.144659 --> 0.137403).  Saving model ...\n",
      "[  7/100] train_loss: 0.19750 valid_loss: 0.13469\n",
      "Validation loss decreased (0.137403 --> 0.134688).  Saving model ...\n",
      "[  8/100] train_loss: 0.18343 valid_loss: 0.12281\n",
      "Validation loss decreased (0.134688 --> 0.122814).  Saving model ...\n",
      "[  9/100] train_loss: 0.17432 valid_loss: 0.11547\n",
      "Validation loss decreased (0.122814 --> 0.115471).  Saving model ...\n",
      "[ 10/100] train_loss: 0.17058 valid_loss: 0.11003\n",
      "Validation loss decreased (0.115471 --> 0.110032).  Saving model ...\n",
      "[ 11/100] train_loss: 0.15810 valid_loss: 0.10612\n",
      "Validation loss decreased (0.110032 --> 0.106122).  Saving model ...\n",
      "[ 12/100] train_loss: 0.15315 valid_loss: 0.10582\n",
      "Validation loss decreased (0.106122 --> 0.105817).  Saving model ...\n",
      "[ 13/100] train_loss: 0.14781 valid_loss: 0.09897\n",
      "Validation loss decreased (0.105817 --> 0.098966).  Saving model ...\n",
      "[ 14/100] train_loss: 0.14538 valid_loss: 0.10105\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[ 15/100] train_loss: 0.14245 valid_loss: 0.09876\n",
      "Validation loss decreased (0.098966 --> 0.098756).  Saving model ...\n",
      "[ 16/100] train_loss: 0.13708 valid_loss: 0.09802\n",
      "Validation loss decreased (0.098756 --> 0.098016).  Saving model ...\n",
      "[ 17/100] train_loss: 0.13421 valid_loss: 0.09599\n",
      "Validation loss decreased (0.098016 --> 0.095988).  Saving model ...\n",
      "[ 18/100] train_loss: 0.12893 valid_loss: 0.09588\n",
      "Validation loss decreased (0.095988 --> 0.095883).  Saving model ...\n",
      "[ 19/100] train_loss: 0.12463 valid_loss: 0.09604\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[ 20/100] train_loss: 0.12444 valid_loss: 0.09299\n",
      "Validation loss decreased (0.095883 --> 0.092995).  Saving model ...\n",
      "[ 21/100] train_loss: 0.12077 valid_loss: 0.09076\n",
      "Validation loss decreased (0.092995 --> 0.090761).  Saving model ...\n",
      "[ 22/100] train_loss: 0.11628 valid_loss: 0.09227\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[ 23/100] train_loss: 0.11624 valid_loss: 0.09231\n",
      "EarlyStopping counter: 2 out of 20\n",
      "[ 24/100] train_loss: 0.11586 valid_loss: 0.09055\n",
      "Validation loss decreased (0.090761 --> 0.090547).  Saving model ...\n",
      "[ 25/100] train_loss: 0.11064 valid_loss: 0.08918\n",
      "Validation loss decreased (0.090547 --> 0.089182).  Saving model ...\n",
      "[ 26/100] train_loss: 0.10822 valid_loss: 0.08855\n",
      "Validation loss decreased (0.089182 --> 0.088546).  Saving model ...\n",
      "[ 27/100] train_loss: 0.10864 valid_loss: 0.08999\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[ 28/100] train_loss: 0.10641 valid_loss: 0.08891\n",
      "EarlyStopping counter: 2 out of 20\n",
      "[ 29/100] train_loss: 0.10285 valid_loss: 0.09042\n",
      "EarlyStopping counter: 3 out of 20\n",
      "[ 30/100] train_loss: 0.10548 valid_loss: 0.08576\n",
      "Validation loss decreased (0.088546 --> 0.085764).  Saving model ...\n",
      "[ 31/100] train_loss: 0.10135 valid_loss: 0.09017\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[ 32/100] train_loss: 0.10028 valid_loss: 0.08757\n",
      "EarlyStopping counter: 2 out of 20\n",
      "[ 33/100] train_loss: 0.09930 valid_loss: 0.09011\n",
      "EarlyStopping counter: 3 out of 20\n",
      "[ 34/100] train_loss: 0.09875 valid_loss: 0.08684\n",
      "EarlyStopping counter: 4 out of 20\n",
      "[ 35/100] train_loss: 0.09639 valid_loss: 0.08770\n",
      "EarlyStopping counter: 5 out of 20\n",
      "[ 36/100] train_loss: 0.09537 valid_loss: 0.08953\n",
      "EarlyStopping counter: 6 out of 20\n",
      "[ 37/100] train_loss: 0.09233 valid_loss: 0.09126\n",
      "EarlyStopping counter: 7 out of 20\n",
      "[ 38/100] train_loss: 0.09496 valid_loss: 0.08873\n",
      "EarlyStopping counter: 8 out of 20\n",
      "[ 39/100] train_loss: 0.09071 valid_loss: 0.08975\n",
      "EarlyStopping counter: 9 out of 20\n",
      "[ 40/100] train_loss: 0.08893 valid_loss: 0.09101\n",
      "EarlyStopping counter: 10 out of 20\n",
      "[ 41/100] train_loss: 0.08903 valid_loss: 0.09039\n",
      "EarlyStopping counter: 11 out of 20\n",
      "[ 42/100] train_loss: 0.09163 valid_loss: 0.09264\n",
      "EarlyStopping counter: 12 out of 20\n",
      "[ 43/100] train_loss: 0.08807 valid_loss: 0.08959\n",
      "EarlyStopping counter: 13 out of 20\n",
      "[ 44/100] train_loss: 0.09179 valid_loss: 0.08726\n",
      "EarlyStopping counter: 14 out of 20\n",
      "[ 45/100] train_loss: 0.08894 valid_loss: 0.09202\n",
      "EarlyStopping counter: 15 out of 20\n",
      "[ 46/100] train_loss: 0.08940 valid_loss: 0.08655\n",
      "EarlyStopping counter: 16 out of 20\n",
      "[ 47/100] train_loss: 0.08817 valid_loss: 0.08352\n",
      "Validation loss decreased (0.085764 --> 0.083516).  Saving model ...\n",
      "[ 48/100] train_loss: 0.08431 valid_loss: 0.08678\n",
      "EarlyStopping counter: 1 out of 20\n",
      "[ 49/100] train_loss: 0.08263 valid_loss: 0.08926\n",
      "EarlyStopping counter: 2 out of 20\n",
      "[ 50/100] train_loss: 0.08337 valid_loss: 0.09035\n",
      "EarlyStopping counter: 3 out of 20\n",
      "[ 51/100] train_loss: 0.07926 valid_loss: 0.08603\n",
      "EarlyStopping counter: 4 out of 20\n",
      "[ 52/100] train_loss: 0.08226 valid_loss: 0.08758\n",
      "EarlyStopping counter: 5 out of 20\n",
      "[ 53/100] train_loss: 0.08132 valid_loss: 0.09059\n",
      "EarlyStopping counter: 6 out of 20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_22540/4176075828.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[0mpatience\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m20\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 10\u001B[1;33m \u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalid_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrain_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbatch_size\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpatience\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mn_epochs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_22540/1080229626.py\u001B[0m in \u001B[0;36mtrain_model\u001B[1;34m(model, batch_size, patience, n_epochs)\u001B[0m\n\u001B[0;32m     38\u001B[0m         \u001B[1;31m######################\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     39\u001B[0m         \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0meval\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m# prep model for evaluation\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 40\u001B[1;33m         \u001B[1;32mfor\u001B[0m \u001B[0mdata\u001B[0m \u001B[1;33m,\u001B[0m \u001B[0mtarget\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mvalid_loader\u001B[0m \u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     41\u001B[0m             \u001B[1;31m# forward pass: 입력된 값을 모델로 전달하여 예측 출력 계산\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     42\u001B[0m             \u001B[0moutput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    519\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_sampler_iter\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    520\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 521\u001B[1;33m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_next_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    522\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_num_yielded\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    523\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataset_kind\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0m_DatasetKind\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mIterable\u001B[0m \u001B[1;32mand\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m_next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    559\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_next_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    560\u001B[0m         \u001B[0mindex\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_next_index\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# may raise StopIteration\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 561\u001B[1;33m         \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataset_fetcher\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfetch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# may raise StopIteration\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    562\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_pin_memory\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    563\u001B[0m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_utils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001B[0m in \u001B[0;36mfetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     42\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mfetch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     43\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mauto_collation\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 44\u001B[1;33m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     45\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     46\u001B[0m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     42\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mfetch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     43\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mauto_collation\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 44\u001B[1;33m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     45\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     46\u001B[0m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\torch\\lib\\site-packages\\torchvision\\datasets\\mnist.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     94\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 95\u001B[1;33m             \u001B[0mimg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     96\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     97\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtarget_transform\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\torch\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001B[0m in \u001B[0;36m__call__\u001B[1;34m(self, pic)\u001B[0m\n\u001B[0;32m     89\u001B[0m             \u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mConverted\u001B[0m \u001B[0mimage\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     90\u001B[0m         \"\"\"\n\u001B[1;32m---> 91\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto_tensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpic\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     92\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     93\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__repr__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\torch\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001B[0m in \u001B[0;36mto_tensor\u001B[1;34m(pic)\u001B[0m\n\u001B[0;32m     77\u001B[0m         \u001B[0mimg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m255\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_numpy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpic\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0muint8\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     78\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 79\u001B[1;33m         \u001B[0mimg\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mByteTensor\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mByteStorage\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrom_buffer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpic\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtobytes\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     80\u001B[0m     \u001B[1;31m# PIL image mode: L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     81\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mpic\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmode\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m'YCbCr'\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mC:\\ProgramData\\Miniconda3\\envs\\torch\\lib\\site-packages\\PIL\\Image.py\u001B[0m in \u001B[0;36mtobytes\u001B[1;34m(self, encoder_name, *args)\u001B[0m\n\u001B[0;32m    718\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfrombytes\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    719\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 720\u001B[1;33m     \u001B[1;32mdef\u001B[0m \u001B[0mtobytes\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencoder_name\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"raw\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    721\u001B[0m         \"\"\"\n\u001B[0;32m    722\u001B[0m         \u001B[0mReturn\u001B[0m \u001B[0mimage\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0ma\u001B[0m \u001B[0mbytes\u001B[0m \u001B[0mobject\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "n_epochs = 100\n",
    "\n",
    "train_loader, test_loader, valid_loader = create_datasets(batch_size)\n",
    "\n",
    "# early stopping patience;\n",
    "# validation loss가 개선된 마지막 시간 이후로 얼마나 기다릴지 지정\n",
    "patience = 20\n",
    "\n",
    "model, train_loss, valid_loss = train_model(model, batch_size, patience, n_epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}